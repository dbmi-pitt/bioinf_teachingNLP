{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work through all of the cells in this notebook and answer the questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I: creating a word embedding using single value decomposition over the 'reuters' corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next several cells load the necessary libraries and define the necessary functions for the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### \n",
    "# Example of SVD on a co-occurrence matrix from https://web.stanford.edu/class/cs224n/assignments/a1_preview/exploring_word_vectors.html\n",
    "#\n",
    "#\n",
    "# All Import Statements Defined Here\n",
    "# Note: Do not add to this list.\n",
    "# All the dependencies you need, can be installed by running .\n",
    "# ----------------\n",
    "\n",
    "import sys\n",
    "assert sys.version_info[0]==3\n",
    "assert sys.version_info[1] >= 5\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "import nltk\n",
    "nltk.download('reuters')\n",
    "from nltk.corpus import reuters\n",
    "import numpy as np\n",
    "import random\n",
    "import scipy as sp\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "START_TOKEN = '<START>'\n",
    "END_TOKEN = '<END>'\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "# ----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(category=\"crude\"):\n",
    "    \"\"\" Read files from the specified Reuter's category.\n",
    "        Params:\n",
    "            category (string): category name\n",
    "        Return:\n",
    "            list of lists, with words from each of the processed files\n",
    "    \"\"\"\n",
    "    files = reuters.fileids(category)\n",
    "    return [[START_TOKEN] + [w.lower() for w in list(reuters.words(f))] + [END_TOKEN] for f in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distinct_words(corpus):\n",
    "    \"\"\" Determine a list of distinct words for the corpus.\n",
    "        Params:\n",
    "            corpus (list of list of strings): corpus of documents\n",
    "        Return:\n",
    "            corpus_words (list of strings): list of distinct words across the corpus, sorted (using python 'sorted' function)\n",
    "            num_corpus_words (integer): number of distinct words across the corpus\n",
    "    \"\"\"\n",
    "    corpus_words = []\n",
    "    num_corpus_words = -1\n",
    "    \n",
    "    vocabD = {}\n",
    "    for l in corpus:\n",
    "        for it in l:             \n",
    "            if not vocabD.get(it):\n",
    "                vocabD[it] = 1\n",
    "            else:\n",
    "                vocabD[it] = vocabD[it] + 1\n",
    "    \n",
    "    corpus_words = [x for x in vocabD.keys()]\n",
    "    corpus_words.sort()\n",
    "    num_corpus_words = len(corpus_words)\n",
    "    return corpus_words, num_corpus_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_co_occurrence_matrix(corpus, window_size=4):\n",
    "    \"\"\" Compute co-occurrence matrix for the given corpus and window_size (default of 4).\n",
    "    \n",
    "        Note: Each word in a document should be at the center of a window. Words near edges will have a smaller\n",
    "              number of co-occurring words.\n",
    "              \n",
    "              For example, if we take the document \"START All that glitters is not gold END\" with window size of 4,\n",
    "              \"All\" will co-occur with \"START\", \"that\", \"glitters\", \"is\", and \"not\".\n",
    "    \n",
    "        Params:\n",
    "            corpus (list of list of strings): corpus of documents\n",
    "            window_size (int): size of context window\n",
    "        Return:\n",
    "            M (numpy matrix of shape (number of corpus words, number of corpus words)): \n",
    "                Co-occurence matrix of word counts. \n",
    "                The ordering of the words in the rows/columns should be the same as the ordering of the words given by the distinct_words function.\n",
    "            word2Ind (dict): dictionary that maps word to index (i.e. row/column number) for matrix M.\n",
    "    \"\"\"\n",
    "    words, num_words = distinct_words(corpus)\n",
    "    M = None\n",
    "    word2Ind = {}\n",
    "    \n",
    "    i = 0\n",
    "    for w in words:\n",
    "        word2Ind[w] = i\n",
    "        i += 1\n",
    "    \n",
    "    M = np.matrix(np.ndarray(shape=(num_words,num_words), dtype=float, buffer=np.array([0.0]*num_words*num_words)))\n",
    "\n",
    "    for l in corpus:\n",
    "        idx = 0\n",
    "        maxIdx = len(l) - 1\n",
    "        for it in l:\n",
    "            curWordIdx = word2Ind[it]\n",
    "            for j in range(1, window_size + 1):\n",
    "                if (idx + j) <= maxIdx:\n",
    "                    targetWordIdx = word2Ind[l[idx + j]]\n",
    "                    M[curWordIdx,targetWordIdx] += 1.0\n",
    "                    M[targetWordIdx,curWordIdx] += 1.0\n",
    "            idx += 1\n",
    "                    \n",
    "    \n",
    "    return M, word2Ind, words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_to_k_dim(M, k=2):\n",
    "    \"\"\" Reduce a co-occurence count matrix of dimensionality (num_corpus_words, num_corpus_words)\n",
    "        to a matrix of dimensionality (num_corpus_words, k) using the following SVD function from Scikit-Learn:\n",
    "            - http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html\n",
    "    \n",
    "        Params:\n",
    "            M (numpy matrix of shape (number of corpus words, number of corpus words)): co-occurence matrix of word counts\n",
    "            k (int): embedding size of each word after dimension reduction\n",
    "        Return:\n",
    "            M_reduced (numpy matrix of shape (number of corpus words, k)): matrix of k-dimensioal word embeddings.\n",
    "                    In terms of the SVD from math class, this actually returns U * S\n",
    "    \"\"\"    \n",
    "    n_iters = 10     # Use this parameter in your call to `TruncatedSVD`\n",
    "    M_reduced = None\n",
    "    print(\"Running Truncated SVD over %i words...\" % (M.shape[0]))\n",
    "    svd = TruncatedSVD(n_components = 2, n_iter = n_iters)\n",
    "    svd.fit(M)\n",
    "    M_reduced = svd.transform(M)\n",
    "    print(M_reduced)\n",
    "\n",
    "    print(\"Done.\")\n",
    "    return M_reduced\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embeddings(M_reduced, word2Ind, words, text_offset = 0.1):\n",
    "    \"\"\" Plot in a scatterplot the embeddings of the words specified in the list \"words\".\n",
    "        NOTE: do not plot all the words listed in M_reduced / word2Ind.\n",
    "        Include a label next to each point.\n",
    "        \n",
    "        Params:\n",
    "            M_reduced (numpy matrix of shape (number of unique words in the corpus , k)): matrix of k-dimensioal word embeddings\n",
    "            word2Ind (dict): dictionary that maps word to indices for matrix M\n",
    "            words (list of strings): words whose embeddings we want to visualize\n",
    "    \"\"\"\n",
    "    xvec = []\n",
    "    yvec = []\n",
    "    wplotted = []\n",
    "    for w in words:\n",
    "        if word2Ind.get(w) is None:\n",
    "            print('ERROR: word ', w, 'not present in the dictionary of words!')\n",
    "            return\n",
    "        \n",
    "        xvec.append(M_reduced[word2Ind[w]][0])\n",
    "        yvec.append(M_reduced[word2Ind[w]][1])\n",
    "        wplotted.append(w)\n",
    "        \n",
    "    plt.scatter(xvec, yvec, marker='x', color='red')\n",
    "    \n",
    "    for i in range(0,len(xvec)):\n",
    "        x = xvec[i]\n",
    "        y = yvec[i]\n",
    "        if i == len(wplotted):\n",
    "            break \n",
    "                \n",
    "        plt.text(x + text_offset, y + text_offset, wplotted[i], fontsize=9)\n",
    "        \n",
    "            \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we load the corpus, create a co-occurrence matrix (initial and very simple word embeddings), reduce the dimensionality using SVD (the target word embeddings), and then plot a few example words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters_corpus = read_corpus()\n",
    "pprint.pprint(reuters_corpus[:3], compact=True, width=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = [\"START All that glitters isn't gold END\".split(\" \"), \"START All's well that ends well END\".split(\" \")]\n",
    "m, d, words = compute_co_occurrence_matrix(test_corpus, window_size=4)\n",
    "print(words)\n",
    "print(m)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Run This Cell to Produce Your Plot\n",
    "# ------------------------------\n",
    "reuters_corpus = read_corpus()\n",
    "M_co_occurrence, word2Ind_co_occurrence,words = compute_co_occurrence_matrix(reuters_corpus)\n",
    "print('The co-occurrence matrix for reuters is of shape: ' + str(M_co_occurrence.shape))\n",
    "M_reduced_co_occurrence = reduce_to_k_dim(M_co_occurrence, k=2)\n",
    "\n",
    "# Rescale (normalize) the rows to make them each of unit-length\n",
    "M_lengths = np.linalg.norm(M_reduced_co_occurrence, axis=1)\n",
    "M_normalized = M_reduced_co_occurrence / M_lengths[:, np.newaxis] # broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['barrels', 'bpd', 'ecuador', 'energy', 'industry', 'kuwait', 'oil', 'output', 'petroleum', 'venezuela']\n",
    "plot_embeddings(M_normalized, word2Ind_co_occurrence, words, text_offset = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['profit', 'war', 'peace', 'energy', 'environment', 'oil', 'petroleum']\n",
    "plot_embeddings(M_normalized, word2Ind_co_occurrence, words, text_offset = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1:  The context window of the co-occurrence matrix above is size 4. In a real word use case, what benefits and potential limitations might there be to using a size of 2? What about a size of 6?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2:  How could text preprocessing potentially benefit or harm the resulting word embeddings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3:  True/False - the SVD produces a 2-dimensional representation of each word in the term dictionary created from the reuters corpus?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4:  How large of a dimension reduction did SVD provide in this example?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part II: using word2vec word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells load word embeddings from word2vec and demonstrate some of their features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "with ZipFile('../w2v_words_pickle.zip', 'r') as zipObj:\n",
    "   # Extract all the contents of zip file in current directory\n",
    "   zipObj.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We access word2vec as a service because the data structure takes up more than 6gigs. \n",
    "## To do that, we start by loading a cached file of all 3,000,000 'words' (tokens)  \n",
    "import pickle\n",
    "f = open('../w2v_words.pickle','rb')\n",
    "words = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now, 30,000 words are randomly sampled \n",
    "import random\n",
    "print(\"Shuffling words ...\")\n",
    "random.shuffle(words)\n",
    "words = words[:30000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## And then, the word embedding vector for each word is retreived from the word2vec service (can take up to 10 minutes)\n",
    "\n",
    "required_words=['barrels', 'bpd', 'ecuador', 'energy', 'industry', 'kuwait', \n",
    "                'oil', 'output', 'petroleum', 'venezuela',\n",
    "                'profit', 'war', 'peace', 'environment'\n",
    "               ]\n",
    "words = set(words).union(set(required_words))\n",
    "print(\"Retreiving vectors for %i words...\" % len(words))\n",
    "\n",
    "import requests, json \n",
    "word2Ind = {}\n",
    "M = []\n",
    "curInd = 0\n",
    "for w in words:\n",
    "    try:\n",
    "        r = requests.get('http://localhost:8000/word2vec?word=' + w)\n",
    "        if r.status_code == 200: \n",
    "            vec = json.loads(r.text)\n",
    "            M.append(vec[\"data\"])\n",
    "            word2Ind[w] = curInd\n",
    "            curInd += 1\n",
    "    except KeyError:\n",
    "        continue\n",
    "print(\"Vectors for the words are now in word2Ind and list M.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert the list to a Matrix\n",
    "M = np.asarray(M,np.float64)\n",
    "M = np.stack(M)\n",
    "print(\"Take note of M.shape: \" + str(M.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Only for the purpose of visualization - reduce the dimensionality using SVD\n",
    "M_reduced = reduce_to_k_dim(M, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['barrels', 'bpd', 'ecuador', 'energy', 'industry', 'kuwait', 'oil', 'output', 'petroleum', 'venezuela']\n",
    "plot_embeddings(M_reduced, word2Ind, words,text_offset = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['profit', 'war', 'peace', 'energy', 'environment', 'oil', 'petroleum']\n",
    "plot_embeddings(M_reduced, word2Ind, words,text_offset = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4: Based on the plots, what is your impression the results of word2vec embedding vs the simple co-occurrence approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5: True/False:  The full 300 dimension word2vec embedding for each word can be used as input to IR and machine learning NLP algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6: Consider the use case of classifying clinical notes e.g., for mentions of adverse drug events. What potential advantage would the word2vec embeddings have over simple preprocessed terms? Embeddings based on simple co-occurrence? Any disadvantages?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
